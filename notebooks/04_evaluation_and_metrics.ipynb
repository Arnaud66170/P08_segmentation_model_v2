{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6427270f",
   "metadata": {},
   "source": [
    "## Notebook : 04_evaluation_and_metrics.ipynb\n",
    "-  Objectif : √âvaluer tous les mod√®les entra√Æn√©s (UNet Mini, VGG16, MobileNetV2...) sur le test set\n",
    "\n",
    "# 1 - Imports, GPU & v√©rifications pr√©liminaires\n",
    "## 1.1 - Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3ef950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "import time\n",
    "import hashlib\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a08958",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß† TensorFlow version :\", tf.__version__)\n",
    "print(\"üîç GPU disponible :\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"üñ•Ô∏è D√©tails des devices :\")\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146401b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a71f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"‚úÖ Allocation m√©moire GPU dynamique activ√©e\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur d'initialisation GPU : {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Aucun GPU d√©tect√© ‚Äî ex√©cution sur CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04c273",
   "metadata": {},
   "source": [
    "## 1.2 - Ajout de src/ au PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0921fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path(\"..\" ).resolve()\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8261adf7",
   "metadata": {},
   "source": [
    "## 1.3 - D√©finition des chemins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca50f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = project_root / \"models\"\n",
    "data_path = project_root / \"data\" / \"processed\" / \"augmented\" / \"test.npz\"\n",
    "outputs_metrics = project_root / \"outputs\" / \"metrics\"\n",
    "outputs_figures = project_root / \"outputs\" / \"figures\"\n",
    "\n",
    "outputs_metrics.mkdir(parents=True, exist_ok=True)\n",
    "outputs_figures.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63202cef",
   "metadata": {},
   "source": [
    "# 2 - V√©rifications pr√©liminaires\n",
    "## 2.1 - Check chemins critiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516aca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.guardrail import check_paths_exist, check_imports\n",
    "check_paths_exist([models_dir, data_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c2b09b",
   "metadata": {},
   "source": [
    "## 2.2 - Check imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc31d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_imports([\"model_training.metrics\", \"utils.viz_utils\"])\n",
    "\n",
    "from model_training.metrics import iou_score, dice_coef\n",
    "from utils.viz_utils import show_image_mask_grid_overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd218a",
   "metadata": {},
   "source": [
    "# 3 - Chargement du jeu de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2a428",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load(data_path)\n",
    "X_test, Y_test = test_data[\"X\"], test_data[\"Y\"]\n",
    "print(f\"‚úÖ Jeu de test charg√© : {X_test.shape}, {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1454aab3",
   "metadata": {},
   "source": [
    "# 4 - √âvaluation des mod√®les entra√Æn√©s\n",
    "## 4.1 - Chargement et √©valuation des mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a16677",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = list(models_dir.glob(\"*.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb2896",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e11aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_path in model_files:\n",
    "    model_name = model_path.stem\n",
    "    print(f\"\\nüîç √âvaluation du mod√®le : {model_name}\")\n",
    "    try:\n",
    "        model = keras.models.load_model(model_path, custom_objects={\"iou_score\": iou_score, \"dice_coef\": dice_coef})\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur de chargement : {e}\")\n",
    "        continue\n",
    "\n",
    "    start = time.time()\n",
    "    metrics = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    end = time.time()\n",
    "\n",
    "    duration = end - start\n",
    "    model_size = model_path.stat().st_size / (1024 * 1024)  # MB\n",
    "    param_count = model.count_params()\n",
    "    hash_val = hashlib.sha256(model_path.read_bytes()).hexdigest()\n",
    "\n",
    "    results.append({\n",
    "        \"model\": model_name,\n",
    "        \"val_loss\": metrics[0],\n",
    "        \"accuracy\": metrics[1],\n",
    "        \"iou_score\": metrics[2],\n",
    "        \"dice_coef\": metrics[3],\n",
    "        \"inference_time\": round(duration / len(X_test), 4),\n",
    "        \"model_size_MB\": round(model_size, 2),\n",
    "        \"params\": param_count,\n",
    "        \"hash\": hash_val[:10]  # extrait pour suivi\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62727d6",
   "metadata": {},
   "source": [
    "# 5 - Synth√®se et visualisations\n",
    "## 5.1 - Tableau comparatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be91ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results).set_index(\"model\")\n",
    "display(results_df.sort_values(by=\"iou_score\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773db22",
   "metadata": {},
   "source": [
    "## 5.2 - Export CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ccf50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(outputs_metrics / \"evaluation_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332d58ae",
   "metadata": {},
   "source": [
    "## 5.3 - Heatmap comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(results_df[[\"iou_score\", \"dice_coef\", \"accuracy\"]], annot=True, fmt=\".3f\", cmap=\"Blues\")\n",
    "plt.title(\"Heatmap des performances mod√®les\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(outputs_figures / \"heatmap_evaluation_scores.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7b3a7",
   "metadata": {},
   "source": [
    "# 6 - Visualisation qualitative\n",
    "## 6.1 - Choix du meilleur mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94a9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = results_df.sort_values(by=\"iou_score\", ascending=False).index[0]\n",
    "print(f\"üèÜ Meilleur mod√®le d√©tect√© : {best_model_name}\")\n",
    "best_model_path = models_dir / f\"{best_model_name}.h5\"\n",
    "best_model = keras.models.load_model(best_model_path, custom_objects={\"iou_score\": iou_score, \"dice_coef\": dice_coef})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29271b79",
   "metadata": {},
   "source": [
    "## 6.2 - Visualisation (5 exemples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    pred = best_model.predict(X_test[i:i+1])\n",
    "    pred_mask = np.argmax(pred[0], axis=-1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axes[0].imshow(X_test[i])\n",
    "    axes[0].set_title(\"Image\")\n",
    "    axes[1].imshow(Y_test[i], cmap=\"nipy_spectral\")\n",
    "    axes[1].set_title(\"Mask R√©el\")\n",
    "    axes[2].imshow(pred_mask, cmap=\"nipy_spectral\")\n",
    "    axes[2].set_title(\"Pr√©diction\")\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outputs_figures / f\"prediction_sample_{i}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17508880",
   "metadata": {},
   "source": [
    "# 7 - G√©n√©ration d'un rapport DOCX\n",
    "## 7.1 - Cr√©ation du document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93619ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3288fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document()\n",
    "doc.add_heading(\"Rapport d'√©valuation P08 - Segmentation\", level=1)\n",
    "doc.add_paragraph(f\"Mod√®le s√©lectionn√© : {best_model_name}\")\n",
    "doc.add_paragraph(f\"Param√®tres : {results_df.loc[best_model_name]['params']} / Taille : {results_df.loc[best_model_name]['model_size_MB']} Mo\")\n",
    "doc.add_paragraph(f\"Hash du mod√®le : {results_df.loc[best_model_name]['hash']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a95b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.add_heading(\"Scores\", level=2)\n",
    "for metric in [\"accuracy\", \"iou_score\", \"dice_coef\"]:\n",
    "    doc.add_paragraph(f\"{metric} : {results_df.loc[best_model_name][metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bba59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.add_picture(str(outputs_figures / \"heatmap_evaluation_scores.png\"), width=Inches(5.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d46fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    img_path = outputs_figures / f\"prediction_sample_{i}.png\"\n",
    "    if img_path.exists():\n",
    "        doc.add_picture(str(img_path), width=Inches(5.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6daf5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.save(outputs_metrics / \"rapport_performance.docx\")\n",
    "print(\"üìÑ Rapport DOCX g√©n√©r√© avec succ√®s.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_p8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
